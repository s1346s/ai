1) 웹 데이터 수집 개요
- 웹 데이터 수집(Web Crawling)은 
  웹 페이지에서 필요한 정보를 자동으로 추출하는 과정.

- 정적(Static) 페이지는 HTML 내에 
  데이터가 포함되어 있으므로 단순 요청만으로 수집 가능.

- 동적(Dynamic) 페이지는 JavaScript 실행 결과로 
  데이터가 생성되므로 Selenium 등의 도구가 필요.


2) HTML 구조와 데이터 추출
- HTML은 계층적 구조로 구성된 문서로, 
  태그(tag), 속성(attribute), 내용(content)으로 이루어짐.

- 원하는 데이터를 추출하기 위해 
  태그 이름, id, class, CSS 선택자 등을 활용.

- BeautifulSoup 라이브러리를 사용해 
  HTML 파싱 및 데이터 선택을 수행.


3) 공공데이터(Open API)
- 정부나 기관이 공개한 데이터를 
  누구나 활용할 수 있도록 제공하는 인터페이스.

- API 호출 시 인증키(serviceKey)가 필요하며, 
  파라미터를 통해 요청 조건을 지정함.

- 응답은 JSON 또는 XML 형태로 제공되며, 
  이를 파싱해 필요한 데이터만 추출.


4) Python을 이용한 공공데이터 수집 개념
- requests 모듈을 이용해 HTTP 요청을 보내고 응답을 받음.

- 응답 데이터는 JSON 형태로 변환 후 
  필요한 필드만 선택해 가공함.

- 일반적인 흐름:
  요청(URL 생성) → 응답 수신 → 데이터 파싱 → 가공 및 저장.
